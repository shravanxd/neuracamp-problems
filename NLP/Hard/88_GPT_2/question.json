{
    "qid": 88,
    "title": "GPT-2 Text Generation",
    "qtext": "# GPT-2 Text Generation\n\n## Description\nYou are tasked with implementing a simplified GPT-2-like text generation function in Python. This function will incorporate the following components of a minimal GPT-2 architecture:\n\n- **Token Embeddings**: Map input tokens to dense vector representations.\n- **Positional Embeddings**: Add positional information to token embeddings.\n- **Multi-head Attention**: Attend to various parts of the sequence.\n- **Feed-Forward Network**: Process attention outputs through a dense layer.\n- **Layer Normalization**: Stabilize the training process.\n\nThe function must take in the following parameters:\n\n- **Prompt**: The initial text to guide the generation process.\n- **Number of Tokens to Generate**: Specify how many tokens to output.\n\nYour function should output the generated text.\n\nAdditionally, utilize the helper function `load_encoder_hparams_and_params` to retrieve:\n\n- A dummy encoder.\n- Model hyperparameters.\n- Model parameters.\n\nBuild your text generation logic around these components.\n\n## Constraints\n- The `prompt` is a non-empty string.\n- $1 \\leq \\text{n\\_tokens\\_to\\_generate} \\leq 100$\n\n## Example 1\n\n**Input:**\n```python\nprompt = \"hello\"\nn_tokens_to_generate = 5\n```\n\n**Output:**\n```python\n\"world <UNK> <UNK> <UNK> <UNK>\"\n```\n\n**Explanation:**\nThe function encodes the input \"hello\" into tokens using the dummy encoder, then runs a simplified GPT-2 forward pass to generate 5 tokens. Finally, it decodes the generated tokens back into text.\n\n## Reasoning\nThis exercise helps understand the core architecture of GPT-2 models, including embeddings, attention mechanisms, and autoregressive token generation. It highlights the role of each architectural component in text generation.",
    "inputFormat": "```python\nprompt: str\nn_tokens_to_generate: int\n```",
    "outputFormat": "```python\nstr\n```",
    "reason": "This exercise helps understand the core architecture of GPT-2 models, including embeddings, attention mechanisms, and autoregressive token generation. It highlights the role of each architectural component in text generation.",
    "learnAbout": "## Learning: GPT-2 Architecture\n\nGPT-2, or Generative Pre-trained Transformer 2, is a state-of-the-art language model developed by OpenAI. It is based on the transformer architecture, which uses self-attention mechanisms to process input data. The model consists of several key components:\n\n- **Token Embeddings**: These are dense vector representations of input tokens, capturing semantic meaning.\n- **Positional Embeddings**: These embeddings provide information about the position of tokens in a sequence, allowing the model to understand the order of words.\n- **Multi-head Attention**: This mechanism allows the model to focus on different parts of the input sequence simultaneously, improving its ability to capture dependencies.\n- **Feed-Forward Networks**: These are fully connected layers that process the output of the attention mechanism, adding non-linearity to the model.\n- **Layer Normalization**: This technique stabilizes the training process by normalizing the inputs of each layer, leading to faster convergence.\n\nUnderstanding these components is crucial for grasping how GPT-2 generates coherent and contextually relevant text."
}