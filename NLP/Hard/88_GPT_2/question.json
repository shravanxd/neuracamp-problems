{
    "qid": 88,
    "title": "GPT-2 Text Generation",
    "qtext": "Implement a Simplified GPT-2-like Text Generation Function\nYou are tasked with implementing a simplified GPT-2-like text generation function in Python. This function will incorporate the following components of a minimal GPT-2 architecture:\n\n- Token Embeddings: Map input tokens to dense vector representations.\n- Positional Embeddings: Add positional information to token embeddings.\n- Multi-head Attention: Attend to various parts of the sequence.\n- Feed-Forward Network: Process attention outputs through a dense layer.\n- Layer Normalization: Stabilize the training process.\n\nThe function must take in the following parameters:\n- Prompt: The initial text to guide the generation process.\n- Number of Tokens to Generate: Specify how many tokens to output.\n\nYour function should output the generated text.\n\nAdditionally, utilize the helper function `load_encoder_hparams_and_params` to retrieve:\n- A dummy encoder.\n- Model hyperparameters.\n- Model parameters.\n\nBuild your text generation logic around these components.\n\nExample:\nInput:\n```python\nprompt = \"hello\"\nn_tokens_to_generate = 5\n```\nOutput:\n```python\nworld <UNK> <UNK> <UNK> <UNK>\n```\n\nReasoning:\nThe function encodes the input \"hello\" into tokens using the dummy encoder, then runs a simplified GPT-2 forward pass to generate 5 tokens. Finally, it decodes the generated tokens back into text.",
    "inputFormat": "Two inputs:\n- `prompt` (str): Initial text prompt for generation.\n- `n_tokens_to_generate` (int): Number of tokens to generate after the prompt.",
    "outputFormat": "One output:\n- A string representing the generated text.",
    "reason": "This exercise helps understand the core architecture of GPT-2 models, including embeddings, attention mechanisms, and autoregressive token generation. It highlights the role of each architectural component in text generation.",
    "learnAbout": "Understanding Transformer Architecture and Text Generation\n\nTransformers have revolutionized the field of Natural Language Processing (NLP) with their efficient and scalable architecture. This guide provides an in-depth look into the core components of transformers and how they facilitate advanced text generation.\n\n**Core Components:**\n- **GELU Activation Function**: Smooth activation that enhances deep networks' training.\n- **Softmax for Attention**: Converts raw attention scores into normalized probabilities.\n- **Layer Normalization**: Stabilizes training by standardizing inputs.\n- **Multi-Head Attention**: Allows the model to focus on different parts of the input sequence simultaneously.\n- **Feedforward Network (FFN)**: Projects embeddings into higher dimensions, applies non-linearity, and projects back.\n- **Transformer Block**: Composed of Multi-Head Attention + FFN + Layer Normalization + Residual connections.\n\n**How GPT-2 Uses These Concepts:**\n- **Token and Positional Embeddings** encode meaning and position.\n- **Causal Attention** ensures that each token can only attend to previous tokens, preserving the autoregressive nature.\n- **Stacked Transformer Blocks** refine hidden representations progressively.\n\n**GPT-2 Text Generation Process:**\n1. Encode the input prompt into tokens.\n2. Embed tokens and add positional information.\n3. Pass through multiple transformer blocks.\n4. Predict the next token probabilities.\n5. Sample or select the next token.\n6. Repeat until the desired number of tokens is generated.\n\nTransformers' architecture enables parallel computation, faster training, and better capturing of long-range dependencies, making models like GPT-2 powerful for coherent and fluent text generation."
  }
  