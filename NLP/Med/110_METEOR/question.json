{
    "qid": 110,
    "title": "Evaluate Translation Quality with METEOR Score",
    "qtext": "Develop a function to compute the METEOR score for evaluating machine translation quality. Given a reference translation and a candidate translation, calculate the score based on unigram matches, precision, recall, F-mean, and a penalty for word order fragmentation.\n\nExample:\nInput:\n```python\nmeteor_score('Rain falls gently from the sky', 'Gentle rain drops from the sky')\n```\nOutput:\n```python\n0.625\n```\n\nReasoning:\nThe function identifies 4 unigram matches ('rain', 'gently'/'gentle', 'from', 'sky'), computes precision (4/6) and recall (4/5), calculates an F-mean, and then applies a small penalty for two chunks.",
    "inputFormat": "Two inputs:\n- `reference` (str): The reference translation string.\n- `candidate` (str): The candidate translation string.",
    "outputFormat": "One output:\n- A float value representing the METEOR score, rounded to three decimal places.",
    "reason": "The METEOR score balances precision, recall, and word order considerations, making it a more human-aligned metric compared to older evaluation methods like BLEU. The score penalizes fragmented matches through the penalty term, favoring translations with contiguous matching sequences.",
    "learnAbout": "Understanding METEOR Score\n\nMETEOR (Metric for Evaluation of Translation with Explicit ORdering) is a metric generally used for machine translation and evaluating the text output of generative AI models. It was designed to address the limitations of earlier metrics like BLEU by considering semantic similarity beyond exact word matches.\n\n**Key Characteristics**\n- Considers both precision and recall.\n- Includes a penalty for fragmented word order.\n- Offers more human-aligned evaluation.\n\n**Steps to Calculate METEOR:**\n\n1. **Tokenization**\nSplit the reference and candidate into individual tokens (words).\n\n2. **Unigram Matching**\nMatch words exactly between the reference and candidate.\n\n3. **Precision and Recall Calculation**\n- Precision = (Number of matches) / (Number of candidate words)\n- Recall = (Number of matches) / (Number of reference words)\n\n4. **F-mean Calculation**\nWeighted harmonic mean of precision and recall:\n\n$$\nF\\_mean = \\frac{Precision \\times Recall}{(\\alpha \\times Precision) + ((1 - \\alpha) \\times Recall)}\n$$\nwhere \\( \\alpha \\approx 0.9 \\).\n\n5. **Chunk Calculation**\nCount the number of contiguous matched word sequences (chunks).\n\n6. **Penalty Calculation**\n\n$$\nPenalty = \\gamma \\times \\left(\\frac{Chunks}{Matches}\\right)^{\\beta}\n$$\nwhere typically \\( \\gamma = 0.5 \\) and \\( \\beta = 3 \\).\n\n7. **Final METEOR Score**\n\n$$\nMETEOR = F\\_mean \\times (1 - Penalty)\n$$\n\n**Applications**\n- Machine Translation Evaluation\n- Text Summarization Evaluation\n- Generative AI output evaluation"
  }
  