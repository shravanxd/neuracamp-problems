{
    "qid": 110,
    "title": "Evaluate Translation Quality with METEOR Score",
    "qtext": "# Evaluate Translation Quality with METEOR Score\n\n## Description\n\nDevelop a function to compute the METEOR score for evaluating machine translation quality. The METEOR score is calculated based on unigram matches between a reference translation and a candidate translation. It incorporates precision, recall, an F-mean, and a penalty for fragmented word order.\n\nGiven:\n- A reference translation string.\n- A candidate translation string.\n\nThe METEOR score is computed as follows:\n\n1. **Unigram Matches**: Identify unigram matches between the reference and candidate translations.\n2. **Precision ($P$)**: $$ P = \\frac{m}{c} $$ where $m$ is the number of matched unigrams, and $c$ is the total number of unigrams in the candidate.\n3. **Recall ($R$)**: $$ R = \\frac{m}{r} $$ where $r$ is the total number of unigrams in the reference.\n4. **F-mean ($F_{mean}$)**: $$ F_{mean} = \\frac{10 \\cdot P \\cdot R}{R + 9 \\cdot P} $$\n5. **Penalty ($p$)**: A penalty is applied based on the number of chunks (contiguous sequences of matches). The penalty is calculated as:\n   $$ p = 0.5 \\left( \\frac{ch}{m} \\right)^3 $$\n   where $ch$ is the number of chunks.\n6. **METEOR Score**: $$ \\text{METEOR} = F_{mean} \\cdot (1 - p) $$\n\nThe final METEOR score is rounded to three decimal places.\n\n## Constraints\n- The input strings consist of lowercase English letters and spaces only.\n- The length of each string is between 1 and 100 words.\n\n## Example 1\n\n**Input:**\n```python\nmeteor_score('Rain falls gently from the sky', 'Gentle rain drops from the sky')\n```\n\n**Output:**\n```python\n0.625\n```\n\n**Explanation:**\nThe function identifies 4 unigram matches ('rain', 'gently'/'gentle', 'from', 'sky'), computes precision (4/6) and recall (4/5), calculates an F-mean, and then applies a small penalty for two chunks.\n\nThe METEOR score balances precision, recall, and word order considerations, making it a more human-aligned metric compared to older evaluation methods like BLEU. The score penalizes fragmented matches through the penalty term, favoring translations with contiguous matching sequences.",
    "inputFormat": "def meteor_score(reference: str, candidate: str) -> float:",
    "outputFormat": "A float value representing the METEOR score, rounded to three decimal places.",
    "reason": "The METEOR score balances precision, recall, and word order considerations, making it a more human-aligned metric compared to older evaluation methods like BLEU. The score penalizes fragmented matches through the penalty term, favoring translations with contiguous matching sequences.",
    "learnAbout": "## METEOR Score\n\nThe METEOR (Metric for Evaluation of Translation with Explicit ORdering) score is a metric used to evaluate the quality of machine translations. Unlike BLEU, which primarily focuses on precision, METEOR incorporates both precision and recall, providing a more balanced evaluation. The F-mean is a harmonic mean of precision and recall, weighted to favor recall. Additionally, METEOR introduces a penalty for fragmented matches, encouraging translations that maintain contiguous sequences of words. This makes METEOR more aligned with human judgment, as it considers both the accuracy and fluency of translations."
}