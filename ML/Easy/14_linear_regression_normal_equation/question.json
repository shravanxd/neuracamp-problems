{
    "qid": 14,
    "title": "Linear Regression Using Normal Equation",
    "qtext": "# Linear Regression Using Normal Equation\n\n## Description\nImplement a Python function to perform linear regression using the normal equation. The function should accept a matrix $X$ (features) and a vector $y$ (target) as inputs, and return the coefficients of the linear regression model. The coefficients should be rounded to four decimal places. Note that a result of $-0.0$ is valid when rounding very small numbers.\n\nThe normal equation for linear regression is given by:\n$$\n\\theta = (X^TX)^{-1}X^Ty\n$$\nwhere $\\theta$ represents the vector of coefficients.\n\n## Constraints\n- $X$ is a 2D list or numpy array with dimensions $m \\times n$, where $m$ is the number of samples and $n$ is the number of features.\n- $y$ is a 1D list or numpy array with $m$ elements.\n- Assume $X$ has full rank, meaning $(X^TX)$ is invertible.\n\n## Example 1\n**Input:**\n```python\nX = [[1, 1], [1, 2], [1, 3]]\ny = [1, 2, 3]\n```\n**Output:**\n```python\n[0.0, 1.0]\n```\n**Explanation:**\nThe linear model is $y = 0.0 + 1.0 \\times x$, which perfectly fits the input data.",
    "inputFormat": "X = [[1, 1], [1, 2], [1, 3]]; y = [1, 2, 3]",
    "outputFormat": "[0.0, 1.0]",
    "reason": "The linear model is y = 0.0 + 1.0*x, perfectly fitting the input data.",
    "learnAbout": "## Learning: Linear Regression and the Normal Equation\nLinear regression is a fundamental statistical method used to model the relationship between a dependent variable $y$ and one or more independent variables $X$. The goal is to find the best-fitting line (or hyperplane in higher dimensions) that minimizes the sum of squared differences between the observed and predicted values.\n\nThe normal equation provides a closed-form solution to the linear regression problem. It is derived from setting the derivative of the cost function (mean squared error) with respect to the coefficients to zero, resulting in the equation $\\theta = (X^TX)^{-1}X^Ty$. This method is computationally efficient for small to medium-sized datasets where $X^TX$ is invertible."
}