{
    "qid": 14,
    "title": "Linear Regression Using Normal Equation",
    "qtext": "Write a Python function that performs linear regression using the normal equation. The function should take a matrix X (features) and a vector y (target) as input, and return the coefficients of the linear regression model. Round your answer to four decimal places, -0.0 is a valid result for rounding a very small number.\n\nExample:\nInput:\nX = [[1, 1], [1, 2], [1, 3]], y = [1, 2, 3]\nOutput:\n[0.0, 1.0]\n\nReasoning:\nThe linear model is y = 0.0 + 1.0*x, perfectly fitting the input data.",
    "inputFormat": "X = [[1, 1], [1, 2], [1, 3]], y = [1, 2, 3]",
    "outputFormat": "[0.0, 1.0]",
    "reason": "The linear model is y = 0.0 + 1.0*x, perfectly fitting the input data.",
    "learnAbout": "Linear Regression Using the Normal Equation\n\nLinear regression aims to model the relationship between a scalar dependent variable ( y ) and one or more explanatory variables (or independent variables) ( X ). The normal equation provides an analytical solution to find the coefficients ( \\theta ) that minimize the cost function for linear regression.\n\nGiven a matrix ( X ) (with each row representing a training example and each column a feature) and a vector ( y ) (representing the target values), the normal equation is:\n\n$$\n\\theta = (X^T X)^{-1} X^T y\n$$\n\n**Explanation of Terms**\n- ( X^T ) is the transpose of ( X ).\n- ( (X^TX)^{-1} ) is the inverse of the matrix ( X^TX ).\n- ( y ) is the vector of target values.\n\n**Key Points**\n- Feature Scaling: This method does not require feature scaling.\n- Learning Rate: There is no need to choose a learning rate.\n- Computational Cost: Computing the inverse of ( X^TX ) can be computationally expensive if the number of features is very large.\n\n**Practical Implementation**\nA practical implementation involves augmenting ( X ) with a column of ones to account for the intercept term and then applying the normal equation directly to compute ( \\theta )."
  }  