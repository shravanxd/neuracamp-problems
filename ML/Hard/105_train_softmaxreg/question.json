{
    "qid": 105,
    "title": "Train Softmax Regression with Gradient Descent",
    "qtext": "Implement a gradient descent-based training algorithm for Softmax regression. Your task is to compute model parameters using Cross Entropy loss and return the optimized coefficients along with collected loss values over iterations. Make sure to round your solution to 4 decimal places.\n\nExample:\nInput:\ntrain_softmaxreg(np.array([[0.5, -1.2], [-0.3, 1.1], [0.8, -0.6]]), np.array([0, 1, 2]), 0.01, 10)\n\nOutput:\n([[-0.0011, 0.0145, -0.0921], [0.002, -0.0598, 0.1263], [-0.0009, 0.0453, -0.0342]], [3.2958, 3.2611, 3.2272, 3.1941, 3.1618, 3.1302, 3.0993, 3.0692, 3.0398, 3.011])\n\nReasoning:\nThe function iteratively updates the Softmax regression parameters using gradient descent and collects loss values over iterations.",
    "inputFormat": "train_softmaxreg(np.array([[0.5, -1.2], [-0.3, 1.1], [0.8, -0.6]]), np.array([0, 1, 2]), 0.01, 10)",
    "outputFormat": "([[-0.0011, 0.0145, -0.0921], [0.002, -0.0598, 0.1263], [-0.0009, 0.0453, -0.0342]], [3.2958, 3.2611, 3.2272, 3.1941, 3.1618, 3.1302, 3.0993, 3.0692, 3.0398, 3.011])",
    "reason": "The function iteratively updates the Softmax regression parameters using gradient descent and collects loss values over iterations.",
    "learnAbout": "Overview\n\nSoftmax regression is a type of logistic regression that extends it to a multiclass problem by outputting a vector \\( P \\) of probabilities for each distinct class and taking \\( \\text{argmax}(P) \\).\n\n**Connection to a regular logistic regression**\n\nRecall that a standard logistic regression is aimed at approximating\n\n$$\np = \\frac{1}{e^{-X\\beta} + 1} = \\frac{e^{X\\beta}}{1 + e^{X\\beta}}\n$$\n\nwhich actually aligns with the definition of the softmax function:\n\n$$\n\\text{softmax}(z_i) = \\sigma(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^C e^{z_j}}\n$$\n\nwhere \\( C \\) is the number of classes. Hence it simply extends the functionality of sigmoid to more than 2 classes and could be used for assigning probability values in a categorical distribution.\n\nSoftmax regression searches for the following vector-approximation:\n\n$$\np(i) = \\frac{e^{x(i)\\beta}}{\\sum_{j=1}^C e^{x(i)\\beta_j}}\n$$\n\n**Loss in softmax regression**\n\ntl;dr: key differences in the loss from logistic regression include replacing sigmoid with softmax and calculating several gradients for vectors \\( \\beta_j \\) corresponding to each class \\( j \\in \\{1, \\dots, C\\} \\).\n\nRecall that we use MLE in logistic regression. It is the same case with softmax regression, although instead of Bernoulli-distributed random variable we have categorical distribution.\n\nIts PMF is defined as:\n\n$$\nf(y|p) = \\prod_{i=1}^K p_i^{[i=y]}\n$$\n\nHence, our log-likelihood looks like:\n\n$$\n\\sum_X \\sum_{j=1}^C [y_i = j] \\log[p(x_i)]\n$$\n\nWhere we replace our probability function with softmax:\n\n$$\n\\sum_X \\sum_{j=1}^C [y_i = j] \\log\\left( \\frac{e^{x_i\\beta_j}}{\\sum_{j=1}^C e^{x_i\\beta_j}} \\right)\n$$\n\nwhere \\( [i=y] \\) is an indicator function that returns 0 if \\( i \\neq y \\), and 1 otherwise.\n\n**Optimization objective**\n\nThe optimization objective is the same as with logistic regression. The function, commonly referred to as Cross Entropy (CE), is:\n\n$$\n\\text{argmin}_\\beta -\\left[ \\sum_X \\sum_{j=1}^C [y_i = j] \\log\\left( \\frac{e^{x_i\\beta_j}}{\\sum_{j=1}^C e^{x_i\\beta_j}} \\right) \\right]\n$$\n\nThen we use a chain rule for calculating the partial derivative of CE with respect to \\( \\beta \\):\n\n$$\n\\frac{\\partial CE}{\\partial \\beta_i(j)} = \\frac{\\partial CE}{\\partial \\sigma} \\frac{\\partial \\sigma}{\\partial [X\\beta(j)]} \\frac{\\partial [X\\beta(j)]}{\\partial \\beta_i(j)}\n$$\n\nWhich is eventually reduced to a matrix form similar to logistic regression gradient:\n\n$$\nX^T (\\sigma(X\\beta(j)) - Y)\n$$\n\nThen we can finally use gradient descent to iteratively update our parameters:\n\n$$\n\\beta_{t+1}(j) = \\beta_t(j) - \\eta \\left[ X^T (\\sigma(X\\beta_t(j)) - Y) \\right]\n$$"
  }
  