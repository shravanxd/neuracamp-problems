{
    "qid": 105,
    "title": "Train Softmax Regression with Gradient Descent",
    "qtext": "## Train Softmax Regression with Gradient Descent\n\n### Description\nImplement a gradient descent-based training algorithm for Softmax regression. Your task is to compute the model parameters using the cross-entropy loss function and return the optimized coefficients along with the collected loss values over iterations. The Softmax regression model is used for multi-class classification problems and is an extension of logistic regression. The parameters should be rounded to 4 decimal places.\n\n### Mathematical Formulation\n\nGiven a dataset with features $X \\in \\mathbb{R}^{n \\times d}$ and labels $y \\in \\{0, 1, \\ldots, k-1\\}^n$, where $n$ is the number of samples, $d$ is the number of features, and $k$ is the number of classes, the Softmax function is defined as:\n\n$$\nP(y = j \\mid \\mathbf{x}; \\mathbf{W}) = \\frac{e^{\\mathbf{w}_j^T \\mathbf{x}}}{\\sum_{i=0}^{k-1} e^{\\mathbf{w}_i^T \\mathbf{x}}}\n$$\n\nThe cross-entropy loss for a single sample is given by:\n\n$$\nL(\\mathbf{W}) = -\\sum_{j=0}^{k-1} y_j \\log(P(y = j \\mid \\mathbf{x}; \\mathbf{W}))\n$$\n\nwhere $\\mathbf{W}$ is the weight matrix of size $d \\times k$.\n\n### Constraints\n- The input feature matrix $X$ is a numpy array of shape $(n, d)$.\n- The input labels $y$ is a numpy array of shape $(n,)$ with integer values $0 \\le y_i < k$.\n- The learning rate is a positive float.\n- The number of iterations is a positive integer.\n\n### Example 1\n\n**Input:**\n```python\ntrain_softmaxreg(np.array([[0.5, -1.2], [-0.3, 1.1], [0.8, -0.6]]), np.array([0, 1, 2]), 0.01, 10)\n```\n\n**Output:**\n```\n([[-0.0011, 0.0145, -0.0921], [0.002, -0.0598, 0.1263], [-0.0009, 0.0453, -0.0342]], [3.2958, 3.2611, 3.2272, 3.1941, 3.1618, 3.1302, 3.0993, 3.0692, 3.0398, 3.011])\n```\n\n**Explanation:**\nThe function iteratively updates the Softmax regression parameters using gradient descent and collects loss values over iterations.",
    "inputFormat": "train_softmaxreg(np.array([[0.5, -1.2], [-0.3, 1.1], [0.8, -0.6]]), np.array([0, 1, 2]), 0.01, 10)",
    "outputFormat": "([[-0.0011, 0.0145, -0.0921], [0.002, -0.0598, 0.1263], [-0.0009, 0.0453, -0.0342]], [3.2958, 3.2611, 3.2272, 3.1941, 3.1618, 3.1302, 3.0993, 3.0692, 3.0398, 3.011])",
    "reason": "The function iteratively updates the Softmax regression parameters using gradient descent and collects loss values over iterations.",
    "learnAbout": "### Softmax Regression and Gradient Descent\nSoftmax regression, also known as multinomial logistic regression, is a generalization of logistic regression to multiple classes. It uses the Softmax function to convert raw scores (logits) into probabilities, which can then be used for classification. The Softmax function ensures that the output probabilities sum to one, making it suitable for multi-class classification tasks.\n\nGradient descent is an optimization algorithm used to minimize the loss function by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In the context of Softmax regression, gradient descent is used to update the model parameters to minimize the cross-entropy loss, which measures the difference between the predicted probabilities and the true class labels."
}