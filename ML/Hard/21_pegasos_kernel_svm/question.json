{
    "qid": 21,
    "title": "Pegasos Kernel SVM Implementation",
    "qtext": "Write a Python function that implements a deterministic version of the Pegasos algorithm to train a kernel SVM classifier from scratch. The function should take a dataset (as a 2D NumPy array where each row represents a data sample and each column represents a feature), a label vector (1D NumPy array where each entry corresponds to the label of the sample), and training parameters such as the choice of kernel (linear or RBF), regularization parameter (lambda), and the number of iterations. Note that while the original Pegasos algorithm is stochastic (it selects a single random sample at each step), this problem requires using all samples in every iteration (i.e., no random sampling). The function should perform binary classification and return the model's alpha coefficients and bias.\n\nExample:\nInput:\ndata = np.array([[1, 2], [2, 3], [3, 1], [4, 1]]), labels = np.array([1, 1, -1, -1]), kernel = 'rbf', lambda_val = 0.01, iterations = 100\n\nOutput:\nalpha = [0.03, 0.02, 0.05, 0.01], b = -0.05\n\nReasoning:\nUsing the RBF kernel, the Pegasos algorithm iteratively updates the weights based on a sub-gradient descent method, taking into account the non-linear separability of the data induced by the kernel transformation.",
    "inputFormat": "data = np.array([[1, 2], [2, 3], [3, 1], [4, 1]]), labels = np.array([1, 1, -1, -1]), kernel = 'rbf', lambda_val = 0.01, iterations = 100",
    "outputFormat": "alpha = [0.03, 0.02, 0.05, 0.01], b = -0.05",
    "reason": "Using the RBF kernel, the Pegasos algorithm iteratively updates the weights based on a sub-gradient descent method, taking into account the non-linear separability of the data induced by the kernel transformation.",
    "learnAbout": "Pegasos Algorithm for Kernel SVM (Deterministic Version)\n\n**Introduction**\n\nThe Pegasos Algorithm (Primal Estimated sub-GrAdient SOlver for SVM) is a fast, iterative algorithm designed to train Support Vector Machines (SVM). While the original Pegasos algorithm uses stochastic updates by selecting one random sample per iteration, this problem requires a deterministic version meaning every data sample is evaluated and considered in each iteration. This deterministic approach ensures reproducibility and clarity, particularly useful for educational purposes.\n\n**Key Concepts**\n\n**Kernel Trick:**\nSVM typically separates data classes using a linear hyperplane. However, real-world data isn't always linearly separable. The Kernel Trick implicitly maps input data into a higher-dimensional feature space, making it easier to separate non-linear data.\n\nCommon kernel functions include:\n\n- **Linear Kernel:**\n\n$$\nK(x,y) = x \\cdot y\n$$\n\n- **Radial Basis Function (RBF) Kernel:**\n\n$$\nK(x,y) = e^{-\\frac{\\|x-y\\|^2}{2\\sigma^2}}\n$$\n\n**Regularization Parameter ( \\( \\lambda \\) ):**\nThis parameter balances how closely the model fits training data against the complexity of the model, helping to prevent overfitting.\n\n**Sub-gradient Descent:**\nPegasos optimizes the SVM objective function using iterative parameter updates based on the sub-gradient of the hinge loss.\n\n**Deterministic Pegasos Algorithm Steps**\n\nGiven training samples \\( (x_i, y_i) \\), labels \\( y_i \\in \\{-1, 1\\} \\), kernel function \\( K \\), regularization parameter \\( \\lambda \\), and total iterations \\( T \\):\n\n1. Initialize alpha coefficients \\( \\alpha_i = 0 \\) and bias \\( b = 0 \\).\n2. For each iteration \\( t = 1, 2, \\dots, T \\):\n   - Compute learning rate: \\( \\eta_t = \\frac{1}{\\lambda t} \\)\n   - For each training sample \\( (x_i, y_i) \\):\n     - Compute decision value:\n\n$$\nf(x_i) = \\sum_j \\alpha_j y_j K(x_j, x_i) + b\n$$\n\n     - If the margin constraint \\( y_i f(x_i) < 1 \\) is violated, update parameters:\n\n$$\n\\alpha_i \\leftarrow \\alpha_i + \\eta_t (y_i - \\lambda \\alpha_i)\n$$\n\n$$\nb \\leftarrow b + \\eta_t y_i\n$$\n\n**Example (Conceptual Explanation)**\n\nConsider a simple dataset:\n\nData:\n\n$$\nX = [[1,2],[2,3],[3,1],[4,1]]\n$$\n\nLabels:\n\n$$\nY = [1,1,-1,-1]\n$$\n\nParameters: Linear kernel, \\( \\lambda = 0.01 \\), iterations = 1.\n\nInitially, parameters (\\( \\alpha, b \\)) start at zero. For each sample, you calculate the decision value. Whenever a sample violates the margin constraint (\\( y_i f(x_i) < 1 \\)), you update the corresponding \\( \\alpha_i \\) and bias \\( b \\) as described. After looping through all samples for the specified iterations, you obtain the trained parameters.\n\n**Important Implementation Notes:**\n- Always iterate through all samples in every iteration (no stochastic/random sampling).\n- Clearly distinguish kernel function choices in your implementation.\n- After training, predictions for new data \\( x \\) are made using:\n\n$$\n\\hat{y}(x) = \\text{sign}\\left( \\sum_j \\alpha_j y_j K(x_j, x) + b \\right)\n$$\n\nThis deterministic Pegasos variant clearly demonstrates how kernelized SVM training operates and simplifies the understanding of kernel methods."
  }
  