{
    "qid": 47,
    "title": "Implement Gradient Descent Variants with MSE Loss",
    "qtext": "### Implement Gradient Descent Variants with MSE Loss\n\nIn this problem, you are required to implement a function that performs three variants of gradient descent: **Stochastic Gradient Descent (SGD)**, **Batch Gradient Descent**, and **Mini-Batch Gradient Descent**. The function should utilize the **Mean Squared Error (MSE)** as the loss function. An additional parameter will specify which variant to use.\n\n#### Mean Squared Error (MSE)\nThe MSE loss function is defined as:\n$$\nMSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n$$\nwhere $y_i$ is the true value and $\\hat{y}_i$ is the predicted value.\n\n#### Gradient Descent Variants\n- **Batch Gradient Descent**: Uses the entire dataset to compute the gradient.\n- **Stochastic Gradient Descent (SGD)**: Uses a single data point to compute the gradient.\n- **Mini-Batch Gradient Descent**: Uses a subset of the dataset to compute the gradient.\n\n#### Constraints\n- $X$ is a numpy array of shape $(m, n)$ where $m$ is the number of samples and $n$ is the number of features.\n- $y$ is a numpy array of shape $(m,)$.\n- $weights$ is a numpy array of shape $(n,)$.\n- $0 < \\text{learning\textunderscore rate} \\le 1$\n- $1 \\le \\text{n\textunderscore iterations} \\le 10^6$\n- $1 \\le \\text{batch\textunderscore size} \\le m$\n\n#### Example 1\n**Input:**\n```python\nimport numpy as np\n\nX = np.array([[1, 1], [2, 1], [3, 1], [4, 1]])\ny = np.array([2, 3, 4, 5])\nweights = np.zeros(X.shape[1])\nlearning_rate = 0.01\nn_iterations = 1000\nbatch_size = 2\nmethod = 'batch'\n```\n**Output:**\n```\n[float, float]\n```\n**Explanation:** The function returns the final weights after performing Batch Gradient Descent on the data.",
    "inputFormat": "```python\nimport numpy as np\n\nX = np.array([[1, 1], [2, 1], [3, 1], [4, 1]])\ny = np.array([2, 3, 4, 5])\nweights = np.zeros(X.shape[1])\nlearning_rate = 0.01\nn_iterations = 1000\nbatch_size = 2\nmethod = 'batch'\n```",
    "outputFormat": "```\n[float, float]\n```",
    "reason": "The function should return the final weights after performing the specified variant of gradient descent.",
    "learnAbout": "### Gradient Descent\nGradient Descent is an optimization algorithm used to minimize the loss function by iteratively moving towards the minimum value. It updates the parameters in the opposite direction of the gradient of the loss function with respect to the parameters. The size of the steps taken is determined by the learning rate.\n\n#### Variants\n- **Batch Gradient Descent**: Computes the gradient using the entire dataset, which can be computationally expensive but provides a stable convergence.\n- **Stochastic Gradient Descent (SGD)**: Computes the gradient using a single data point, which introduces noise into the optimization process but can lead to faster convergence.\n- **Mini-Batch Gradient Descent**: A compromise between Batch and Stochastic, using a small subset of the dataset to compute the gradient, offering a balance between convergence speed and stability."
}