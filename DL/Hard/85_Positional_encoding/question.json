{
    "qid": 85,
    "title": "Positional Encoding Calculator",
    "qtext": "# Positional Encoding Calculator\n\n## Description\nImplement a Python function to compute the Positional Encoding for a Transformer model. The function should calculate positional encodings for a given sequence length $\\text{position}$ and model dimensionality $d_{\\text{model}}$ using sine and cosine functions as specified in the Transformer architecture. The function should return $-1$ if $\\text{position} = 0$ or if $d_{\\text{model}} \\leq 0$. The output should be a numpy array of type `float16`.\n\nThe positional encoding is computed as follows:\n\nFor each position $pos$ and dimension $i$:\n\n- If $i$ is even:\n  $$ PE_{(pos, i)} = \\sin\\left(\\frac{pos}{10000^{\\frac{i}{d_{\\text{model}}}}}\\right) $$\n- If $i$ is odd:\n  $$ PE_{(pos, i)} = \\cos\\left(\\frac{pos}{10000^{\\frac{i-1}{d_{\\text{model}}}}}\\right) $$\n\n## Constraints\n- $1 \\leq \\text{position} \\leq 10^4$\n- $1 \\leq d_{\\text{model}} \\leq 512$\n- Input values are integers.\n\n## Example 1\n\n**Input:**\n```python\nposition = 2\nd_model = 8\n```\n\n**Output:**\n```python\narray([[[ 0., 0., 0., 0., 1., 1., 1., 1.],\n        [ 0.8413, 0.0998, 0.01, 0.001, 0.5405, 0.995, 1., 1.]]], dtype=float16)\n```\n\n**Explanation:**\nThe function computes the positional encoding by calculating sine values for even indices and cosine values for odd indices, ensuring that the encoding provides the required positional information following the Transformer design.",
    "inputFormat": "```python\nposition: int\nd_model: int\n```",
    "outputFormat": "```python\nnumpy.ndarray of shape (1, position, d_model) with dtype float16\n```",
    "reason": "Positional encodings allow the Transformer model to incorporate information about the order of tokens without recurrence or convolution. Sine functions are applied to even indices and cosine functions to odd indices to generate a unique encoding for each position.",
    "learnAbout": "## Positional Encoding in Transformers\n\nPositional encoding is a technique used in Transformer models to provide information about the position of tokens in a sequence. Unlike recurrent or convolutional neural networks, Transformers do not inherently understand the order of input tokens. Positional encodings are added to the input embeddings to introduce this sequential information.\n\nThe encoding uses sine and cosine functions of different frequencies to generate a unique positional vector for each token. This approach allows the model to distinguish between different positions in the sequence, enabling it to capture the order of tokens effectively. The choice of sine and cosine functions ensures that the positional encodings are smooth and continuous, which helps in learning complex patterns in the data."
}