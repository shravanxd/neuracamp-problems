{
    "qid": 94,
    "title": "Implement Multi-Head Attention",
    "qtext": "Implement the multi-head attention mechanism, a critical component of transformer models. Given Query (Q), Key (K), and Value (V) matrices, compute the attention outputs for multiple heads and concatenate the results.\n\nExample:\nInput:\nQ = np.array([[1, 0], [0, 1]]), K = np.array([[1, 0], [0, 1]]), V = np.array([[1, 0], [0, 1]]), n_heads = 2\n\nOutput:\n[[1., 0.], [0., 1.]]\n\nReasoning:\nMulti-head attention is computed for 2 heads using the input Q, K, and V matrices. The resulting outputs for each head are concatenated to form the final attention output.",
    "inputFormat": "Q = np.array([[1, 0], [0, 1]]), K = np.array([[1, 0], [0, 1]]), V = np.array([[1, 0], [0, 1]]), n_heads = 2",
    "outputFormat": "[[1., 0.], [0., 1.]]",
    "reason": "Multi-head attention is computed for 2 heads using the input Q, K, and V matrices. The resulting outputs for each head are concatenated to form the final attention output.",
    "learnAbout": "Understanding Multi-Head Attention\n\n**Concepts**\n\nThe attention mechanism allows the model to weigh the importance of different input elements based on their relevance to a specific task. Multi-head attention extends this concept by using multiple attention heads, each learning different representations of the input data, which improves the model's ability to capture richer relationships and dependencies.\n\n**Structure of Multi-Head Attention**\n\nGiven Query (Q), Key (K), and Value (V) matrices of shape \\((seqLen, d_{model})\\), multi-head attention involves splitting these into \\(n\\) smaller heads each with dimension \\(d_k\\) where:\n\n$$\nd_k = \\frac{d_{model}}{n}\n$$\n\n**Steps:**\n\n1. **Splitting Q, K, and V:**\nEach head operates on a slice of Q, K, and V independently.\n\n2. **Computing Attention for Each Head:**\nThe attention scores for each head are computed as:\n\n$$\nscore_i = \\frac{Q_i K_i^T}{\\sqrt{d_k}}\n$$\n\nApply softmax with numerical stability:\n\n$$\nSoftmaxScore_i = \\text{softmax}(score_i - \\max(score_i))\n$$\n\nEach head's output is:\n\n$$\nhead_i = SoftmaxScore_i \\times V_i\n$$\n\n3. **Softmax Calculation and Numerical Stability:**\nSubtracting the maximum score before applying the exponential prevents numerical overflow without changing the softmax results.\n\n4. **Concatenation and Final Output:**\nThe final multi-head attention output is formed by concatenating the outputs from each head:\n\n$$\nMultiHeadOutput = \\text{concat}(head_1, head_2, ..., head_n)\n$$\n\n**Key Points:**\n- Each attention head processes the input independently.\n- Outputs of all heads are concatenated.\n- This allows the model to learn richer representations by focusing on different relationships in parallel."
  }
  