{
    "qid": 94,
    "title": "Implement Multi-Head Attention",
    "qtext": "# Implement Multi-Head Attention\n\n## Description\n\nImplement the multi-head attention mechanism, a fundamental component of transformer models used in natural language processing and other machine learning tasks. Given matrices for Query ($Q$), Key ($K$), and Value ($V$), compute the attention outputs for multiple heads and concatenate the results to produce the final output.\n\nThe multi-head attention mechanism can be mathematically described as follows:\n\n1. **Scaled Dot-Product Attention**: For each head, compute the attention scores using the formula:\n   $$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V $$\n   where $d_k$ is the dimension of the key vectors.\n\n2. **Multi-Head Attention**: Given $n_{\\text{heads}}$ heads, compute the attention for each head and concatenate the results:\n   $$ \\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\text{head}_2, \\ldots, \\text{head}_{n_{\\text{heads}}})W^O $$\n   where $W^O$ is the output projection matrix.\n\n## Constraints\n- $Q$, $K$, and $V$ are numpy arrays with shapes $(n, d_q)$, $(n, d_k)$, and $(n, d_v)$ respectively.\n- $n_{\\text{heads}}$ is an integer such that $d_q$, $d_k$, and $d_v$ are divisible by $n_{\\text{heads}}$.\n- $n \\geq 1$, $d_q, d_k, d_v \\geq 1$.\n\n## Example 1\n\n**Input:**\n```python\nQ = np.array([[1, 0], [0, 1]])\nK = np.array([[1, 0], [0, 1]])\nV = np.array([[1, 0], [0, 1]])\nn_heads = 2\n```\n\n**Output:**\n```python\n[[1., 0.], [0., 1.]]\n```\n\n**Explanation:**\nMulti-head attention is computed for 2 heads using the input $Q$, $K$, and $V$ matrices. The resulting outputs for each head are concatenated to form the final attention output.",
    "inputFormat": "Q = np.array([[1, 0], [0, 1]])\nK = np.array([[1, 0], [0, 1]])\nV = np.array([[1, 0], [0, 1]])\nn_heads = 2",
    "outputFormat": "[[1., 0.], [0., 1.]]",
    "reason": "Multi-head attention is computed for 2 heads using the input Q, K, and V matrices. The resulting outputs for each head are concatenated to form the final attention output.",
    "learnAbout": "## Multi-Head Attention\n\nMulti-head attention is a mechanism that allows the model to focus on different parts of the input sequence simultaneously. It is a key component of transformer models, which have revolutionized the field of natural language processing. The idea is to project the input data into multiple subspaces, compute attention separately in each subspace, and then combine the results. This approach enables the model to capture various types of relationships and dependencies within the data. By using multiple heads, the model can attend to information from different representation subspaces at different positions, enhancing its ability to understand complex patterns and dependencies."
}