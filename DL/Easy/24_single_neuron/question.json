{
    "qid": 24,
    "title": "Single Neuron",
    "qtext": "Write a Python function that simulates a single neuron with a sigmoid activation function for binary classification, handling multidimensional input features. The function should take a list of feature vectors (each vector representing multiple features for an example), associated true binary labels, and the neuron's weights (one for each feature) and bias as input. It should return the predicted probabilities after sigmoid activation and the mean squared error between the predicted probabilities and the true labels, both rounded to four decimal places.\n\nExample:\nInput:\n```python\nfeatures = [[0.5, 1.0], [-1.5, -2.0], [2.0, 1.5]]\nlabels = [0, 1, 0]\nweights = [0.7, -0.4]\nbias = -0.1\n```\nOutput:\n```python\n([0.4626, 0.4134, 0.6682], 0.3349)\n```\n\nReasoning:\nFor each input vector, the weighted sum is calculated by multiplying each feature by its corresponding weight, adding these up along with the bias, then applying the sigmoid function to produce a probability. The MSE is calculated as the average squared difference between each predicted probability and the corresponding true label.",
    "inputFormat": "Four inputs:\n- A list of lists representing feature vectors.\n- A list of binary labels (0 or 1).\n- A list of neuron weights (one weight per feature).\n- A bias term (float).",
    "outputFormat": "One output:\n- A tuple containing:\n  - A list of predicted probabilities (rounded to 4 decimal places).\n  - The mean squared error between predictions and true labels (rounded to 4 decimal places).",
    "reason": "The neuron combines inputs linearly with weights and bias, applies the sigmoid activation to produce probabilities, and the mean squared error evaluates the prediction quality against the true labels.",
    "learnAbout": "This task models a single neuron used for binary classification with multidimensional input features.\n\n**Mathematical Background:**\n\n- Neuron output:\n```math\nz = \\sum (weight_i \\times feature_i) + bias\n```\n- Sigmoid activation:\n```math\n\\sigma(z) = \\frac{1}{1 + e^{-z}}\n```\n- Mean Squared Error (MSE):\n```math\nMSE = \\frac{1}{n} \\sum (predicted - true)^2\n```\n\n**Explanation of Terms:**\n- z: Weighted sum plus bias.\n- \\sigma(z): Sigmoid function output (probability).\n- MSE: Measures average squared difference between predicted probabilities and true labels.\n\n**Applications:**\n- Logistic regression.\n- Neural network binary classifiers.\n- Introduction to backpropagation and optimization tasks.\n\nThis problem strengthens understanding of forward passes, activation functions, and loss calculations in neural networks."
  }
  