{
    "qid": 44,
    "title": "Leaky ReLU Activation Function",
    "qtext": "# Leaky ReLU Activation Function\n\n## Description\nImplement the Leaky Rectified Linear Unit (Leaky ReLU) activation function in Python. The function should be named `leaky_relu` and accept a float `z` as input, along with an optional float `alpha` that defaults to 0.01. The Leaky ReLU function is defined as:\n\n\\[\n\\text{Leaky ReLU}(z) = \\begin{cases} \n  z, & \\text{if } z \\ge 0 \\\\\n  \\alpha z, & \\text{if } z < 0\n\\end{cases}\n\\]\n\nThe function should return the computed value after applying the Leaky ReLU transformation.\n\n## Example 1\n**Input:**\n```python\nprint(leaky_relu(0))\nprint(leaky_relu(1))\nprint(leaky_relu(-1))\nprint(leaky_relu(-2, alpha=0.1))\n```\n**Output:**\n```\n0\n1\n-0.01\n-0.2\n```\n**Explanation:**\n- For \\( z = 0 \\), the output is 0 because \\( z \\ge 0 \\).\n- For \\( z = 1 \\), the output is 1 because \\( z \\ge 0 \\).\n- For \\( z = -1 \\), the output is \\(-0.01\\) because \\( \\alpha \\cdot z = 0.01 \\cdot (-1) \\).\n- For \\( z = -2 \\) with \\( \\alpha = 0.1 \\), the output is \\(-0.2\\) because \\( \\alpha \\cdot z = 0.1 \\cdot (-2) \\).\n\n## Constraints\n- \\(-1000 \\le z \\le 1000\\)\n- \\(0 \\le \\alpha \\le 1\\)\n- Input is a float.\n\n",
    "inputFormat": "print(leaky_relu(z))",
    "outputFormat": "float",
    "reason": "For z = 0, the output is 0.\nFor z = 1, the output is 1.\nFor z = -1, the output is -0.01 (0.01 * -1).\nFor z = -2 with alpha = 0.1, the output is -0.2 (0.1 * -2).",
    "learnAbout": "The Leaky Rectified Linear Unit (Leaky ReLU) is an activation function used in artificial neural networks. Unlike the standard ReLU, which outputs zero for negative inputs, the Leaky ReLU allows a small, non-zero, constant gradient \\( \\alpha \\) when the unit is not active. This helps mitigate the problem of dying neurons during training, where neurons can become inactive and stop learning entirely. The Leaky ReLU is defined mathematically as: \\[ \\text{Leaky ReLU}(z) = \\begin{cases} z, & \\text{if } z \\ge 0 \\\\ \\alpha z, & \\text{if } z < 0 \\end{cases} \\] This function is particularly useful in deep learning models, as it can improve convergence and model performance."
}