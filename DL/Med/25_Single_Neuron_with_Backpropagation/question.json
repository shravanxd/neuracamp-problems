{
    "qid": 25,
    "title": "Single Neuron with Backpropagation",
    "qtext": "## Single Neuron with Backpropagation\n\nImplement a Python function to simulate a single neuron with a sigmoid activation function. The function should perform backpropagation to update the neuron's weights and bias using gradient descent based on the Mean Squared Error (MSE) loss. The function should take as input a list of feature vectors, corresponding binary labels, initial weights, an initial bias, a learning rate, and the number of epochs. The function should return the updated weights, updated bias, and a list of MSE values for each epoch, rounded to four decimal places.\n\n### Mathematical Formulation\n\nGiven a feature vector $\\mathbf{x} = [x_1, x_2, \\ldots, x_n]$ and weights $\\mathbf{w} = [w_1, w_2, \\ldots, w_n]$, the neuron computes the weighted sum $z$ as:\n\n$$ z = \\mathbf{w} \\cdot \\mathbf{x} + b $$\n\nThe sigmoid activation function is applied to $z$ to obtain the prediction $\\hat{y}$:\n\n$$ \\hat{y} = \\sigma(z) = \\frac{1}{1 + e^{-z}} $$\n\nThe MSE loss for a single prediction is given by:\n\n$$ L(\\hat{y}, y) = \\frac{1}{2}(\\hat{y} - y)^2 $$\n\nThe gradients of the loss with respect to the weights $\\mathbf{w}$ and bias $b$ are computed and used to update the parameters:\n\n- Gradient with respect to weights: $$ \\frac{\\partial L}{\\partial \\mathbf{w}} = (\\hat{y} - y) \\cdot \\hat{y} \\cdot (1 - \\hat{y}) \\cdot \\mathbf{x} $$\n- Gradient with respect to bias: $$ \\frac{\\partial L}{\\partial b} = (\\hat{y} - y) \\cdot \\hat{y} \\cdot (1 - \\hat{y}) $$\n\nThe weights and bias are updated using the learning rate $\\alpha$:\n\n$$ \\mathbf{w} = \\mathbf{w} - \\alpha \\cdot \\frac{\\partial L}{\\partial \\mathbf{w}} $$\n$$ b = b - \\alpha \\cdot \\frac{\\partial L}{\\partial b} $$\n\n### Constraints\n- Each feature vector and initial weights are lists of floats.\n- Labels are binary (0 or 1).\n- Learning rate $\\alpha$ is a positive float.\n- Number of epochs is a positive integer.\n\n### Example 1\n\n**Input:**\n```python\nfeatures = [[1.0, 2.0], [2.0, 1.0], [-1.0, -2.0]]\nlabels = [1, 0, 0]\ninitial_weights = [0.1, -0.2]\ninitial_bias = 0.0\nlearning_rate = 0.1\nepochs = 2\n```\n\n**Output:**\n```python\nupdated_weights = [0.1036, -0.1425]\nupdated_bias = -0.0167\nmse_values = [0.3033, 0.2942]\n```\n\n**Explanation:**\nThe function iteratively updates the weights and bias using backpropagation over 2 epochs. The MSE values for each epoch are calculated and returned along with the updated parameters.",
    "inputFormat": "features = [[1.0, 2.0], [2.0, 1.0], [-1.0, -2.0]]\nlabels = [1, 0, 0]\ninitial_weights = [0.1, -0.2]\ninitial_bias = 0.0\nlearning_rate = 0.1\nepochs = 2",
    "outputFormat": "updated_weights = [0.1036, -0.1425]\nupdated_bias = -0.0167\nmse_values = [0.3033, 0.2942]",
    "reason": "The neuron receives feature vectors and computes predictions using the sigmoid activation. Based on the predictions and true labels, the gradients of MSE loss with respect to weights and bias are computed and used to update the model parameters across epochs.",
    "learnAbout": "### Sigmoid Activation and Gradient Descent\n\nThe sigmoid function, $\\sigma(z) = \\frac{1}{1 + e^{-z}}$, is a common activation function used in neural networks. It maps any real-valued number into the range (0, 1), making it suitable for binary classification problems. The sigmoid function's derivative, $\\sigma'(z) = \\sigma(z)(1 - \\sigma(z))$, is used in backpropagation to compute gradients efficiently.\n\nGradient descent is an optimization algorithm used to minimize the loss function by iteratively updating the model parameters. In this problem, the Mean Squared Error (MSE) is used as the loss function. The gradients of the loss with respect to the weights and bias are calculated using the chain rule, and the parameters are updated in the direction that reduces the loss, scaled by the learning rate."
}