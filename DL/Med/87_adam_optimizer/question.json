{
    "qid": 87,
    "title": "Adam Optimizer",
    "qtext": "## Adam Optimizer\n\nImplement the Adam optimizer update step function. The Adam optimizer is an adaptive learning rate optimization algorithm designed to handle sparse gradients on noisy problems. Your task is to implement a function that updates the parameter values using the Adam optimization algorithm.\n\n### Function Signature\n```python\ndef adam_optimizer_update(parameter, grad, m, v, t, alpha=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n    pass\n```\n\n### Description\nThe Adam optimizer updates the parameters using the following equations:\n\n1. **Update biased first moment estimate**:\n   $$ m_t = \\beta_1 \\cdot m_{t-1} + (1 - \\beta_1) \\cdot g_t $$\n\n2. **Update biased second raw moment estimate**:\n   $$ v_t = \\beta_2 \\cdot v_{t-1} + (1 - \\beta_2) \\cdot g_t^2 $$\n\n3. **Compute bias-corrected first moment estimate**:\n   $$ \\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t} $$\n\n4. **Compute bias-corrected second raw moment estimate**:\n   $$ \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t} $$\n\n5. **Update parameters**:\n   $$ \\theta_t = \\theta_{t-1} - \\alpha \\cdot \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon} $$\n\nWhere:\n- $\\theta_t$ is the parameter at time $t$.\n- $g_t$ is the gradient at time $t$.\n- $\\alpha$ is the learning rate.\n- $\\beta_1$ and $\\beta_2$ are the exponential decay rates for the moment estimates.\n- $\\epsilon$ is a small constant to prevent division by zero.\n\n### Constraints\n- $-10^5 \\le \\text{parameter}, \\text{grad}, m, v \\le 10^5$\n- $1 \\le t \\le 10^6$\n- Inputs can be scalars or numpy arrays.\n\n### Example 1\n**Input:**\n```python\nparameter = 1.0\ngrad = 0.1\nm = 0.0\nv = 0.0\nt = 1\n```\n**Output:**\n```python\n(0.999, 0.01, 0.0001)\n```\n**Explanation:**\nWith the given inputs, the Adam optimizer updates the parameter from 1.0 to 0.999 using the computed bias-corrected estimates of the first and second moments.",
    "inputFormat": "parameter, grad, m, v, t",
    "outputFormat": "(updated_parameter, updated_m, updated_v)",
    "reason": "The Adam optimizer computes updated values for the parameter, first moment (m), and second moment (v) using bias-corrected estimates of gradients. With input values parameter=1.0, grad=0.1, m=0.0, v=0.0, and t=1, the updated parameter becomes 0.999.",
    "learnAbout": "### Adam Optimizer\nThe Adam optimizer is a popular optimization algorithm used in training machine learning models. It combines the advantages of two other extensions of stochastic gradient descent: the Adaptive Gradient Algorithm (AdaGrad) and Root Mean Square Propagation (RMSProp). Adam computes adaptive learning rates for each parameter by maintaining estimates of both the first and second moments of the gradients. The first moment is essentially the mean of the gradients, while the second moment is the uncentered variance. By using these moments, Adam adjusts the learning rate for each parameter, which helps in converging faster and avoiding oscillations. The bias-correction steps are crucial, especially during the initial stages of training, to ensure that the moments do not start with zero values, which could lead to incorrect updates."
}