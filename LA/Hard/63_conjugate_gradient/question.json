{
    "qid": 63,
    "title": "Implement the Conjugate Gradient Method for Solving Linear Systems",
    "qtext": "Task: Implement the Conjugate Gradient Method for Solving Linear Systems\n\nYour task is to implement the Conjugate Gradient (CG) method, an efficient iterative algorithm for solving large, sparse, symmetric, positive-definite linear systems. Given a matrix A and a vector b, the algorithm will solve for x in the system (Ax = b).\n\nWrite a function conjugate_gradient(A, b, n, x0=None, tol=1e-8) that performs the Conjugate Gradient method as follows:\n\nA: A symmetric, positive-definite matrix representing the linear system.\nb: The vector on the right side of the equation.\nn: Maximum number of iterations.\nx0: Initial guess for the solution vector.\ntol: Tolerance for stopping criteria.\n\nThe function should return the solution vector x.\n\nExample:\n\nInput:\n```python\nA = np.array([[4, 1], [1, 3]])\nb = np.array([1, 2])\nn = 5\n\nprint(conjugate_gradient(A, b, n))\n```\n\nOutput:\n```python\n[0.09090909, 0.63636364]\n```\n\nReasoning:\nThe Conjugate Gradient method is applied to the linear system Ax = b with the given matrix A and vector b. The algorithm iteratively refines the solution to converge to the exact solution.",
    "inputFormat": "Five inputs:\n- A (NumPy array): Symmetric, positive-definite 2D array.\n- b (NumPy array): 1D array representing the right-hand side vector.\n- n (integer): Maximum number of iterations.\n- x0 (optional NumPy array): Initial guess (default is zero vector).\n- tol (float): Tolerance for convergence (default is 1e-8).",
    "outputFormat": "A 1D NumPy array representing the approximate solution vector x.",
    "reason": "The Conjugate Gradient method is efficient for solving symmetric positive-definite systems, providing faster convergence compared to methods like steepest descent by ensuring search directions are A-orthogonal.",
    "learnAbout": "Understanding The Conjugate Gradient Method\n\nThe Conjugate Gradient (CG) method is an iterative algorithm used to solve large systems of linear equations, particularly those that are symmetric and positive-definite.\n\n**Concepts**\n\nThe CG gradient method is often applied to the quadratic form of a linear system, \\(Ax = b\\):\n\n$$\nf(x) = \\frac{1}{2}x^T A x - b^T x\n$$\n\nThe quadratic form's differential reduces to:\n\n$$\nf'(x) = A x - b\n$$\n\nTherefore, the solution \\(x\\) satisfies \\(Ax = b\\) at the optimum.\n\nThe conjugate gradient method uses search directions that are conjugate to all previous search directions, meaning:\n\n$$\np_i^T A p_j = 0 \\quad \\text{for} \\quad i \\neq j\n$$\n\nThis ensures efficient information gathering along search directions without redundancy, unlike steepest descent.\n\n**Algorithm Steps**\n\n**Initialization:**\n- \\(x_0\\): Initial guess for the variable vector.\n- \\(r_0 = b - A x_0\\): Initial residual vector.\n- \\(p_0 = r_0\\): Initial search direction.\n\n**Iteration (k-th step):**\n- Step size:\n\n$$\n\\alpha_k = \\frac{r_k^T r_k}{p_k^T A p_k}\n$$\n\n- Update solution:\n\n$$\nx_{k+1} = x_k + \\alpha_k p_k\n$$\n\n- Update residual:\n\n$$\nr_{k+1} = r_k - \\alpha_k A p_k\n$$\n\n- Check convergence:\n\n$$\n\\|r_{k+1}\\| < \\text{tolerance}\n$$\n\n- New direction scaling:\n\n$$\n\\beta_k = \\frac{r_{k+1}^T r_{k+1}}{r_k^T r_k}\n$$\n\n- Update search direction:\n\n$$\np_{k+1} = r_{k+1} + \\beta_k p_k\n$$\n\n**Termination:**\nStop when \\(\\|r_{k+1}\\| < \\text{tolerance}\\) or maximum iterations are reached.\n\n**Example Calculation**\n\nSolve the system:\n\n$$\n4x_1 + x_2 = 6\n$$\n$$\nx_1 + 3x_2 = 6\n$$\n\nInitialize:\n- \\(x_0 = [0, 0]^T\\)\n- \\(r_0 = [6, 6]^T\\)\n- \\(p_0 = [6, 6]^T\\)\n\n**First Iteration:**\n- Compute \\(\\alpha_0\\):\n\n$$\n\\alpha_0 = \\frac{r_0^T r_0}{p_0^T A p_0} = \\frac{72}{324} = 0.2222\n$$\n\n- Update \\(x_1\\):\n\n$$\nx_1 = [1.3333, 1.3333]^T\n$$\n\n- Update \\(r_1\\):\n\n$$\nr_1 = [6.67, 5.33]^T\n$$\n\n- Compute \\(\\beta_0\\): approximately 0.99\n- Update \\(p_1\\):\n\n$$\np_1 = [12.60, 11.26]^T\n$$\n\n**Second Iteration:**\nCompute \\(\\alpha_1\\), \\(x_2\\), \\(r_2\\), etc., until convergence.\n\n**Applications**\n\n- Optimization\n- Machine Learning\n- Computational Fluid Dynamics\n- Finite Element Methods"
  }
  