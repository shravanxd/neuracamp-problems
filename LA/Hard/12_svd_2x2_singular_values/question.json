{
    "qid": 12,
    "title": "Singular Value Decomposition (SVD)",
    "qtext": "Write a Python function that approximates the Singular Value Decomposition on a 2Ã—2 matrix by using the Jacobi method and without using numpy svd function. (You could, but you wouldn't learn anything.) Return the result in this format.\n\nExample:\n\nInput:\n```python\na = [[2, 1], [1, 2]]\n```\n\nOutput:\n```python\n(array([[-0.70710678, -0.70710678],\n        [-0.70710678,  0.70710678]]),\n array([3., 1.]),\n array([[-0.70710678, -0.70710678],\n        [-0.70710678,  0.70710678]]))\n```\n\nReasoning:\nU is the first matrix, Sigma is the second vector, and V is the third matrix.",
    "inputFormat": "One input:\n- `a` (list of lists): A 2x2 matrix for which to compute the SVD.",
    "outputFormat": "A tuple containing three elements:\n- A 2x2 array (U matrix)\n- A 1D array (Singular values vector)\n- A 2x2 array (V matrix)",
    "reason": "Using the Jacobi method to approximate SVD reveals how the matrix can be decomposed into orthogonal matrices and a diagonal matrix without relying on built-in high-level functions, enhancing conceptual understanding.",
    "learnAbout": "Singular Value Decomposition (SVD) via the Jacobi Method\n\nSingular Value Decomposition (SVD) is a powerful matrix decomposition technique in linear algebra that expresses a matrix as the product of three other matrices, revealing its intrinsic geometric and algebraic properties. When using the Jacobi method, SVD decomposes a matrix \\( A \\) into:\n\n$$\nA = U \\Sigma V^T\n$$\n\nwhere:\n\n1. \\( A \\) is the original \\( m \\times n \\) matrix.\n2. \\( U \\) is an \\( m \\times m \\) orthogonal matrix whose columns are the left singular vectors of \\( A \\).\n3. \\( \\Sigma \\) is an \\( m \\times n \\) diagonal matrix containing the singular values of \\( A \\).\n4. \\( V^T \\) is the transpose of an \\( n \\times n \\) orthogonal matrix whose columns are the right singular vectors of \\( A \\).\n\n**The Jacobi Method for SVD**\n\nThe Jacobi method is an iterative algorithm used for diagonalizing a symmetric matrix through a series of rotational transformations. It is particularly suited for computing the SVD by iteratively applying rotations to minimize off-diagonal elements until the matrix is diagonal.\n\n**Steps of the Jacobi SVD Algorithm**\n\n1. **Initialization**: Start with \\( A^T A \\) (or \\( AA^T \\) for \\( U \\)) and set \\( V \\) (or \\( U \\)) as an identity matrix. The goal is to diagonalize \\( A^T A \\), obtaining \\( V \\) in the process.\n\n2. **Choosing Rotation Targets**: Identify off-diagonal elements in \\( A^T A \\) to be minimized or zeroed out through rotations.\n\n3. **Calculating Rotation Angles**: For each target off-diagonal element, calculate the angle \\( \\theta \\) for the Jacobi rotation matrix \\( J \\) that would zero it. Solve for \\( \\theta \\) using atan2 to handle rotation quadrants accurately:\n\n$$\n\\theta = 0.5 \\times \\text{atan2}(2a_{ij}, a_{ii} - a_{jj})\n$$\n\n4. **Applying Rotations**: Construct \\( J \\) using \\( \\theta \\) and apply the rotation to \\( A^T A \\), reducing the magnitude of the target off-diagonal element. Update \\( V \\) (or \\( U \\)) by multiplying it by \\( J \\).\n\n5. **Iteration and Convergence**: Repeat the process of selecting off-diagonal elements, calculating rotation angles, and applying rotations until \\( A^T A \\) is sufficiently diagonalized.\n\n6. **Extracting SVD Components**: Once diagonalized, the diagonal entries of \\( A^T A \\) represent the squared singular values of \\( A \\). The matrices \\( U \\) and \\( V \\) are constructed from the accumulated rotations, containing the left and right singular vectors of \\( A \\), respectively.\n\n**Practical Considerations**\n\n1. The Jacobi method is particularly effective for dense matrices where off-diagonal elements are significant.\n2. Careful implementation is required to ensure numerical stability and efficiency, especially for large matrices.\n3. The iterative nature of the Jacobi method makes it computationally intensive, but it is highly parallelizable."
  }  